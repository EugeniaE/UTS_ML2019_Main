{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "A1_ReportDraft.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EugeniaE/UTS_ML2019_Main/blob/master/A1_Report.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZkcpQJzyrQk",
        "colab_type": "text"
      },
      "source": [
        "# Review Report on \"Long Short-Term Memory\" paper \n",
        "by Evgenia Evdokimova 13000738"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhJUYGmyrQl",
        "colab_type": "text"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS9ek0L17qeu",
        "colab_type": "text"
      },
      "source": [
        "This report addresses assignment 1 task in Machine Learning 32513 and briefly outlines the main findings from the paper chosen. \n",
        "\n",
        "The interest to the topic can be explained as LSTM being one of the widely used RNN for time-series prediction, speech analysis which is related to my personal and professional area of interest. The understanding of how and why it works would allow me to use it effectively for resolving of the existing problems.\n",
        "\n",
        "The report can be also found here https://github.com/EugeniaE/UTS_ML2019_Main/blob/master/A1_Report.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgsl4mOzyrQm",
        "colab_type": "text"
      },
      "source": [
        "## Content"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3MqLNRU71ag",
        "colab_type": "text"
      },
      "source": [
        "The main conceptual idea of the paper is to propose a new type of Recurrent neural network called Long Short-Term Memory network. \n",
        " \n",
        "One part of the article includes an overview of the previously existing approaches on recurrent nets with time-dependent input. Authors describe with reference to previous research papers main issues related to recurrent neural networks and the nature of these issues.  The major of them is exponentially decaying error or vanishing error flow. In a few words once an error occurred in a random unit it will scale up dramatically or vanish  with each training step, that makes training uneffective. Authors also mention such consequences of these errors as oscillating weights, extremely long training time or not-effective training that does not improve the results.\n",
        "\n",
        "As a potential solution and main idea of the paper the authors propose LSTM where the central feature is a constant error carrousel (CEC). This feature allows to protect memory content stored in memory units from perturbation by irrelevant data. Authors give visual representation of their complex structure element called a memory cell. Each memory cell in LSTM includes input and output gate units, where input and output gate units will have to learn which errors to trap or when to release them in CEC by scaling them appropriately. \n",
        "\n",
        "Half of the article describes  6 experiments aimed to prove the effectiveness of proposed LSTM network and comparative results table is given at the end.  The results are quite ambiguous and not obvious. Conceptually authors prove the effectiveness of this approach and its potential power by showing successful experiments with some tasks. Still quite a big section describes the limitations of proposed LSTM and potential problems, so the idea is still to be further researched as of the time of publications. \n",
        "\n",
        "We know that currently LSTM is one of the widely used networks so we can say that at some extent the authors` effort to resolve exponentially decaying error flow by representing idea of LSTM was quite valuable. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GqM8vOpyrQn",
        "colab_type": "text"
      },
      "source": [
        "## Innovation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frfWDaQW9piL",
        "colab_type": "text"
      },
      "source": [
        "The paper seems to be quite innovative for the time of publication (1997) even though its usability was not clear yet at that period of time. For scientists of that time it could be a great reading material and may seem as something new that potentially could resolve not just vanishing error problem back then. Experiments 4-6 proposed that LSTM will be able to solve long time lag problems with continuous-valued representations, the task has never previously been solved by other RNN. \n",
        "\n",
        "The authors describe new type of neural network that will include constant error carousel approach (CEC). Network architecture will include memory cells with input and output gate units. This units will learn how to handle irrelevant inputs by putting weight (0-1) on them or scaling them appropriately. In the experiments taken the authors showcase some benefits and some cases where and how this approach works with different tasks.\n",
        "\n",
        "I have some feeling that the description is a little brief in some parts and very theoretical, and authors did not apply their technique to any practical problems yet. Nowadays there are many resources describing LSTM clearly in details, and additionally some modifications and improvements were made since that. One of this updates is forget gate that was not named so in the article, but was further explained in later works. Basically, authors propose new type of neural network that can use data from previous time steps and within special memory cells put different weights on data that needs to be “remembered” or “forgotten”.  \n",
        "\n",
        "Authors did good job trying to apply LSTM to different tasks although these tasks did not represent real life problems. \n",
        "\n",
        "There is definitely an innovative approach and after 20 years we see that it was very successful as LSTM is widely used. Today the idea seems to be not that complicated, and many scientists managed to describe it in a much simpler way. But the impact of this idea is still massive as even with modern and very deep arcitectures of neural networks that exist today LSTM still demonstrates very good results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOGSgQkMyrQo",
        "colab_type": "text"
      },
      "source": [
        "## Technical quality"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICZKwL1k-Q9l",
        "colab_type": "text"
      },
      "source": [
        "The paper is of very high technical quality with description of experiments, arguments and results. Although the feeling is that experiments can hardly be repeated. For example, in experiment 1 authors just mention randomly generated pairs of training and test sets for learning “embedded Reber grammar”. Second experiment includes 3 sub experiments without full context and just giving input hyperparameters (for example LSTM, RTRL, BPTT compared with different learning rate, that makes results not obvious and can change with opposite change of learning rate among networks). In some experiments there is not enough context on why choosing this task or these paramethers. Some results described as successful, some as failed. All results have numerical confirmation\n",
        "\n",
        "The results table following the experiments does not demonstrate any straightforward ideas and quite big section with limitations of LSTM just confirms that even with big number of different experiments there are still many problems and this new proposed LSTM does not resolve all the existing ones.\n",
        "\n",
        "From technical perspective it is good that authors included algorithm details into the appendix, but in my understanding that it is not self-exploratory enough mostly with descriptive formulas that can hardly be understood without other references. Therefore, it makes it really hard to see the innovation based just on the paper and requires very solid background in mathematics, statistics, neural networks or extra reading.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgcFAcShyrQp",
        "colab_type": "text"
      },
      "source": [
        "## Application and X-factor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCtfRq5NAcHF",
        "colab_type": "text"
      },
      "source": [
        "As used datasets were generated randomly it is hard to say how this algorithm can help with real life problems.  In the conclusion authors say that they just intend to apply it to real world data. This means they acknowledge the fact that their experiments were very theoretical and real application is yet to be done. Again, they say that the application areas will include time series prediction, music composition and speech processing, but at the moment of publication  it was just a suggestion rather than a real solution.\n",
        "\n",
        "\n",
        "From my own experience and based on internet research I can see that proposed LSTM with some corrections-improvements is widely used currently for time-series predictions and for speech processing. This fact makes me think that even with not fully clear and highly theoretical paper authors could propose useful and quite good-working idea.  As it is more than 20 years since paper was published and LSTM was explored, applied and explained by many scientists in much a better way I would not recommend this paper as a learning material due to very high level of technical details that do not necessarily have any practical value nowadays. Could be valuable back in 1997 though.\n",
        "\n",
        "For suggestions I would recommend authors to apply their algorithm to real world data to give readers better context and better understanding of the problem and more practical advice on its application.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxnYf1woyrQq",
        "colab_type": "text"
      },
      "source": [
        "## Presentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n26Yfk09CrBE",
        "colab_type": "text"
      },
      "source": [
        "The presentation of the topic seems to be very overcomplicated. Even though the idea behind LSTM is not that complex I had to put a lot of effort into understanding of the paper. If saying same things in simpler way it could attract more attention.\n",
        "\n",
        "Some arguments and experiments descriptions are not explained enough. Authors just give the parameters and results without giving enough context why choosing these parameters.  Looks more like a journal of experiments where you write for yourself or people of exactly same level of expertise. This makes the content very difficult for reading and hard to judge or evaluate the quality of the experiments.\n",
        "\n",
        "I believe the authors have very high level of expertise, and we know that LSTM is still extremely popular, but anyway as they say at the very end, they just intended to apply it to real world data. This fact makes this paper highly theoretical, not obvious in conclusions, but still quite innovative.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kuRwnM31yrQr",
        "colab_type": "text"
      },
      "source": [
        "## References\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PiLqkSpDzmq",
        "colab_type": "text"
      },
      "source": [
        "1. Hochreiter, S., Schmidhuber, J. & Elvezia, C. 1997, 'LONG SHORT-TERM MEMORY', *Neural Computation*, vol. 9, no. 8, pp. 1735-80.\n",
        "\n",
        "2. Gers, F.A., Schmidhuber, J. & Cummins, F. 2000, 'Learning to forget: Continual prediction with LSTM', *Neural Computation*, vol. 12, no. 10, pp. 2451-71.\n",
        "  \n",
        "3. 2015, 'Understanding LSTM Networks', colah`s blog, weblog, Christopher Olah, 27 August, viewed 20 August 2019, <<https://colah.github.io/posts/2015-08-Understanding-LSTMs/>>\n",
        "\n"
      ]
    }
  ]
}